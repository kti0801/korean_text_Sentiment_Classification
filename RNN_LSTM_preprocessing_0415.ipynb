{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from soynlp.normalizer import *\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/kdt-admin/data/final_sentiment_dialogues.csv', index_col = 0)\n",
    "\n",
    "# 중복 문장 제거\n",
    "data.drop_duplicates(subset=['문장'], inplace=True)\n",
    "data.reset_index(drop=True, inplace = True)\n",
    "\n",
    "# 감정 object를 수치형 object로 변경\n",
    "data.loc[(data['감정'] == \"중립\"), '감정'] = 0  \n",
    "data.loc[(data['감정'] == \"슬픔\"), '감정'] = 1  \n",
    "data.loc[(data['감정'] == \"분노\"), '감정'] = 2  \n",
    "data.loc[(data['감정'] == \"불안\"), '감정'] = 3  \n",
    "data.loc[(data['감정'] == \"행복\"), '감정'] = 4  \n",
    "data.loc[(data['감정'] == \"당황\"), '감정'] = 5  \n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<*>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def remove_noise(text):\n",
    "    # 이모지 제거\n",
    "    text = re.sub(r'[^\\!\\?\\.\\~\\w\\sㄱ-ㅎㅏ-ㅣ가-힣]', '', text)\n",
    "    return text\n",
    "\n",
    "#'문장' 열에 함수 적용\n",
    "data['문장'] = data['문장'].apply(remove_html_tags)\n",
    "data['문장'] = data['문장'].apply(remove_noise)\n",
    "data[:5]\n",
    "\n",
    "def custom_normalize(text, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ㅋ의 반복에 ㅌ이 섞여있는 경우 ㅋ으로 변환\n",
    "custom_mapping = {\"ㅌ\": \"ㅋ\"}\n",
    "\n",
    "data['문장'] = data['문장'].apply(lambda x: custom_normalize(x, custom_mapping)) # ㅋㅋㅋㅌㅋㅋ 같은 ㅌ이 중간에 껴있는 경우 ㅌ을 ㅋ으로 대체\n",
    "data['문장'] = data['문장'].apply(lambda x: repeat_normalize(x, num_repeats=3)) # ㅋㅋㅋㅋㅋㅋㅋ 같은 반복되는 문자열 ㅋㅋㅋ로 정규화\n",
    "\n",
    "\n",
    "y = data['감정']\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "mecab = Mecab()\n",
    "# 불용어 제거하고 형태소를 분리하는 작업\n",
    "x = []\n",
    "\n",
    "for sentence in data['문장']:\n",
    "    x.append([word for word in mecab.morphs(sentence) if not word in stopwords])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['문장'], y, test_size = 0.2, \n",
    "                                                    random_state = 42)\n",
    "\n",
    "y_train = y_train.astype('float64')\n",
    "y_test = y_test.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN과 LSTM 공통 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 14:36:50.373425: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-14 14:36:50.499757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:50.499775: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 14:36:51.121014: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:51.121083: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:51.121089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 138932 entries, 62730 to 121958\n",
      "Series name: 문장\n",
      "Non-Null Count   Dtype \n",
      "--------------   ----- \n",
      "138932 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 각 단어에 고유한 정수 인덱스 할당\n",
    "tokenizer.fit_on_texts(data['문장'])\n",
    "train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "test_seq = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩\n",
    "max_len = 80\n",
    "train_pad = sequence.pad_sequences(train_seq, maxlen = max_len)\n",
    "test_pad = sequence.pad_sequences(test_seq, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y_train = to_categorical(y_train, num_classes = 6)\n",
    "# y_test = to_categorical(y_test, num_classes = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235424"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 128)         30134272  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 128)               32896     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,175,814\n",
      "Trainable params: 30,175,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 14:36:57.411403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-14 14:36:57.411612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:57.411669: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:57.411706: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:57.411741: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:57.453556: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-04-14 14:36:57.453777: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-04-14 14:36:57.454279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#모델 설계, 구축\n",
    "model_rnn1 = Sequential()\n",
    "model_rnn1.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128))\n",
    "\n",
    "#순환층\n",
    "model_rnn1.add(SimpleRNN(128, return_sequences=False)) \n",
    "model_rnn1.add(Flatten())\n",
    "\n",
    "#Dense 층\n",
    "model_rnn1.add(Dense(64, activation = 'relu'))\n",
    "model_rnn1.add(Dense(6, activation = 'softmax'))\n",
    "\n",
    "model_rnn1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#컴파일\n",
    "model_rnn1.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', \n",
    "                   metrics = ['accuracy'])\n",
    "\n",
    "# 모델 적용\n",
    "model_rnn1.fit(train_pad, y_train, epochs = 10, batch_size = 64, validation_split=0.2, shuffle = True)\n",
    "\n",
    "# 손실 계산\n",
    "loss1 = model_rnn1.evaluate(test_pad, y_test, verbose = 0)\n",
    "print(f'Test Loss : {loss1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "max_features = len(train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = 1, epochs = 10, batch_size = 128\n",
    "model_lstm1 = models.Sequential()\n",
    "model_lstm1.add(layers.Embedding(max_features, 32))\n",
    "\n",
    "model_lstm1.add(layers.LSTM(32))  \n",
    "\n",
    "model_lstm1.add(layers.Dense(6, activation = 'softmax'))\n",
    "\n",
    "model_lstm1.compile(optimizer=RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model_lstm1.fit(train_pad, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#손실 계산\n",
    "test_loss1, test_acc1 = model_lstm1.evaluate(test_pad,y_test)\n",
    "print('Test acc:', test_acc1)\n",
    "print(f'Test Loss : {test_loss1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "y_pred_lstm1 = model_lstm1.predict(test_pad)\n",
    "# 다중 출력 모델의 출력을 다중 클래스 형식으로 변환\n",
    "y_pred_classes_lstm = np.argmax(y_pred_lstm1, axis=1)\n",
    "\n",
    "# 다중 클래스 형식으로 변환된 예측 결과와 실제 레이블 간의 F1-score 계산\n",
    "f1_score_lstm1 = f1_score(y_test, y_pred_classes_lstm, average='weighted')\n",
    "\n",
    "print(f'F1_Score : {f1_score_lstm1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = 2, epochs = 30, batch_size = 128\n",
    "model_lstm2 = models.Sequential()\n",
    "model_lstm2.add(layers.Embedding(max_features, 32))\n",
    "\n",
    "model_lstm2.add(layers.LSTM(32, return_sequences=True))  \n",
    "model_lstm2.add(layers.LSTM(32))  \n",
    "\n",
    "model_lstm2.add(layers.Dense(6, activation = 'softmax'))\n",
    "\n",
    "model_lstm2.compile(optimizer=RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model_lstm2.fit(train_pad, y_train, epochs=30, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 계산\n",
    "test_loss_2, test_acc_2 = model_lstm_2.evaluate(test_pad_tf,y_test_encoded)\n",
    "print('Test acc:', test_acc_2)\n",
    "print(f'Test Loss : {test_loss_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "y_pred_lstm2 = model_lstm2.predict(test_pad)\n",
    "\n",
    "# 다중 출력 모델의 출력을 다중 클래스 형식으로 변환\n",
    "y_pred_classes_lstm2 = np.argmax(y_pred_lstm1, axis=1)\n",
    "\n",
    "# 다중 클래스 형식으로 변환된 예측 결과와 실제 레이블 간의 F1-score 계산\n",
    "f1_score_lstm2 = f1_score(y_test, y_pred_classes_lstm2, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
