{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zh7joaUOZ8as"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/home/kdt-admin/cleaned_data.csv').dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6O0HLJWf_fB",
        "outputId": "6eb435b6-e74a-4ac7-a113-043415f3c480"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(173247, 6)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['문장'].nunique(), data['감정'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ChCCPE_Panbj"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(subset=['문장'], inplace=True)\n",
        "data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx7iBvpqr3CY"
      },
      "source": [
        "## 토큰화 (Kobert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using cached model. /home/kdt-admin/.cache/kobert_v1.zip\n",
            "using cached model. /home/kdt-admin/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 768])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from kobert import get_pytorch_kobert_model\n",
        "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "model, vocab  = get_pytorch_kobert_model()\n",
        "sequence_output, pooled_output = model(input_ids, input_mask, token_type_ids)\n",
        "pooled_output.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n",
              "        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n",
              "        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence_output[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "predefined_args = {\n",
        "        'attention_cell': 'multi_head',\n",
        "        'num_layers': 12,\n",
        "        'units': 768,\n",
        "        'hidden_size': 3072,\n",
        "        'max_length': 512,\n",
        "        'num_heads': 12,\n",
        "        'scaled': True,\n",
        "        'dropout': 0.1,\n",
        "        'use_residual': True,\n",
        "        'embed_size': 768,\n",
        "        'embed_dropout': 0.1,\n",
        "        'token_type_vocab_size': 2,\n",
        "        'word_embed': None,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n",
        "    bertmodel = BertModel.from_pretrained(model_path, return_dict=False)\n",
        "    device = torch.device(ctx)\n",
        "    bertmodel.to(device)\n",
        "    bertmodel.eval()\n",
        "    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(\n",
        "        vocab_file, padding_token=\"[PAD]\"\n",
        "    )\n",
        "    return bertmodel, vocab_b_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Optional\n",
        "from transformers.tokenization_utils import AddedToken\n",
        "from transformers import XLNetTokenizer\n",
        "from transformers import SPIECE_UNDERLINE\n",
        "\n",
        "\n",
        "class KoBERTTokenizer(XLNetTokenizer):\n",
        "    padding_side = \"right\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        do_lower_case=False,\n",
        "        remove_space=True,\n",
        "        keep_accents=False,\n",
        "        bos_token=\"[CLS]\",\n",
        "        eos_token=\"[SEP]\",\n",
        "        unk_token=\"[UNK]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "        additional_special_tokens=None,\n",
        "        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        # Mask token behave like a normal word, i.e. include the space before it\n",
        "        mask_token = (\n",
        "            AddedToken(mask_token, lstrip=True, rstrip=False)\n",
        "            if isinstance(mask_token, str)\n",
        "            else mask_token\n",
        "        )\n",
        "\n",
        "        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n",
        "\n",
        "        super().__init__(\n",
        "            vocab_file,\n",
        "            do_lower_case=do_lower_case,\n",
        "            remove_space=remove_space,\n",
        "            keep_accents=keep_accents,\n",
        "            bos_token=bos_token,\n",
        "            eos_token=eos_token,\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            sp_model_kwargs=self.sp_model_kwargs,\n",
        "            **kwargs,\n",
        "        )\n",
        "        self._pad_token_type_id = 0\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
        "        adding special tokens. An XLNet sequence has the following format:\n",
        "        - single sequence: ``<cls> X <sep>``\n",
        "        - pair of sequences: ``<cls> A <sep> B <sep>``\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of IDs to which the special tokens will be added.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return cls + token_ids_0 + sep\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize a string.\"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "        pieces = self.sp_model.encode(text, out_type=str, **self.sp_model_kwargs)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(\n",
        "                    piece[:-1].replace(SPIECE_UNDERLINE, \"\")\n",
        "                )\n",
        "                if (\n",
        "                    piece[0] != SPIECE_UNDERLINE\n",
        "                    and cur_pieces[0][0] == SPIECE_UNDERLINE\n",
        "                ):\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
        "        adding special tokens. An XLNet sequence has the following format:\n",
        "\n",
        "        - single sequence: ``<cls> X <sep> ``\n",
        "        - pair of sequences: ``<cls> A <sep> B <sep>``\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of IDs to which the special tokens will be added.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return cls + token_ids_0 + sep\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLNet\n",
        "        sequence pair mask has the following format:\n",
        "\n",
        "        ::\n",
        "\n",
        "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
        "            | first sequence    | second sequence |\n",
        "\n",
        "        If :obj:`token_ids_1` is :obj:`None`, this method only returns the first portion of the mask (0s).\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of IDs.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
        "            sequence(s).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.8.1+cu111\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n",
            "NVIDIA A16-16Q\n",
            "True\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# 현재 사용 가능한 디바이스 정보 출력\n",
        "print(torch.cuda.device_count())  # 사용 가능한 CUDA 디바이스 개수\n",
        "print(torch.cuda.current_device())  # 현재 CUDA 디바이스의 인덱스\n",
        "print(torch.cuda.get_device_name(torch.cuda.current_device()))  # 현재 CUDA 디바이스의 이름\n",
        "print(torch.cuda.is_available())  # CUDA 사용 가능 여부\n",
        "\n",
        "# 현재 디바이스 설정\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)  # 현재 디바이스 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.loc[(data['감정'] == \"중립\"), '감정'] = 0  \n",
        "data.loc[(data['감정'] == \"슬픔\"), '감정'] = 1  \n",
        "data.loc[(data['감정'] == \"분노\"), '감정'] = 2  \n",
        "data.loc[(data['감정'] == \"불안\"), '감정'] = 3  \n",
        "data.loc[(data['감정'] == \"행복\"), '감정'] = 4  \n",
        "data.loc[(data['감정'] == \"당황\"), '감정'] = 5  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for ques, label in zip(data['문장'], data['감정'])  :\n",
        "    data = []   \n",
        "    data.append(ques)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n",
        "                 pad, pair):\n",
        "   \n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
        "        \n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "         \n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['너는', '내년', '대선', '때', '투표', '##할', '수', '있어', '?']\n",
            "9311\n",
            "[[2, 9039, 3], [2, 8946, 3], [2, 9311, 3], [2, 1003, 3], [2, 8701, 3], [2, 7, 7, 3358, 3], [2, 1931, 3], [2, 8749, 3], [2, 32, 3]]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "# kcbert의 tokenizer와 모델을 불러옴.\n",
        "kcbert_tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")\n",
        "kcbert = AutoModelForMaskedLM.from_pretrained(\"beomi/kcbert-base\")\n",
        "\n",
        "result = kcbert_tokenizer.tokenize(\"너는 내년 대선 때 투표할 수 있어?\")\n",
        "print(result)\n",
        "print(kcbert_tokenizer.vocab['대선'])\n",
        "print([kcbert_tokenizer.encode(token) for token in result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁너', '는', '▁내년', '▁대선', '▁때', '▁투표', '할', '▁수', '▁있어', '?']\n",
            "[[1457, 3, 2], [517, 5760, 3, 2], [1437, 3, 2], [1654, 3, 2], [1844, 3, 2], [4772, 3, 2], [4977, 3, 2], [2872, 3, 2], [3868, 3, 2], [633, 3, 2]]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "kobert_tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False)\n",
        "\n",
        "result = kobert_tokenizer.tokenize(\"너는 내년 대선 때 투표할 수 있어?\")\n",
        "print(result)\n",
        "kobert_vocab = kobert_tokenizer.get_vocab()\n",
        "print([kobert_tokenizer.encode(token) for token in result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting parameters\n",
        "max_len = 75\n",
        "batch_size = 16\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 2\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "tok=tokenizer.tokenize\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test,0, 1, tok, vocab,  max_len, True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 텍스트 시퀀스의 길이를 저장할 리스트\n",
        "lengths = []\n",
        "\n",
        "# 훈련 데이터셋에 있는 모든 텍스트 시퀀스의 길이 계산\n",
        "for i in range(len(dataset_train)):\n",
        "    # 토큰화\n",
        "    tokens = tok(dataset_train[i][0])\n",
        "    # 길이 저장\n",
        "    lengths.append(len(tokens))\n",
        "\n",
        "# 히스토그램으로 길이 분포 시각화\n",
        "plt.hist(lengths, bins=50, alpha=0.7, color='b')\n",
        "plt.title('Text Length Distribution')\n",
        "plt.xlabel('Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f7de2195518>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=6,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        " \n",
        "#optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "    \n",
        "train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa1244c24f294ce5a90041b9145aa473",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8663 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 batch id 1 loss 1.810744047164917 train acc 0.125\n",
            "epoch 1 batch id 201 loss 1.6681848764419556 train acc 0.2658582089552239\n",
            "epoch 1 batch id 401 loss 1.629830002784729 train acc 0.3725062344139651\n",
            "epoch 1 batch id 601 loss 1.4746519327163696 train acc 0.44124376039933444\n",
            "epoch 1 batch id 801 loss 1.1626346111297607 train acc 0.4887640449438202\n",
            "epoch 1 batch id 1001 loss 0.7883192300796509 train acc 0.5237887112887113\n",
            "epoch 1 batch id 1201 loss 1.017348289489746 train acc 0.550895087427144\n",
            "epoch 1 batch id 1401 loss 0.8342302441596985 train acc 0.5706638115631691\n",
            "epoch 1 batch id 1601 loss 0.8192316889762878 train acc 0.5854153653966271\n",
            "epoch 1 batch id 1801 loss 1.0767757892608643 train acc 0.5976887840088839\n",
            "epoch 1 batch id 2001 loss 1.145171880722046 train acc 0.6061656671664168\n",
            "epoch 1 batch id 2201 loss 0.7734811305999756 train acc 0.6142662426169923\n",
            "epoch 1 batch id 2401 loss 1.4024641513824463 train acc 0.6213817159516868\n",
            "epoch 1 batch id 2601 loss 0.6482945084571838 train acc 0.6294213763936948\n",
            "epoch 1 batch id 2801 loss 0.9589821100234985 train acc 0.6348402356301321\n",
            "epoch 1 batch id 3001 loss 0.6914252042770386 train acc 0.6407030989670109\n",
            "epoch 1 batch id 3201 loss 0.3810095191001892 train acc 0.6456966572945955\n",
            "epoch 1 batch id 3401 loss 1.145517110824585 train acc 0.649864010585122\n",
            "epoch 1 batch id 3601 loss 0.7032718062400818 train acc 0.6536552346570397\n",
            "epoch 1 batch id 3801 loss 0.7542755007743835 train acc 0.6574750065772165\n",
            "epoch 1 batch id 4001 loss 0.6002341508865356 train acc 0.661693951512122\n",
            "epoch 1 batch id 4201 loss 0.831296980381012 train acc 0.6653624137110212\n",
            "epoch 1 batch id 4401 loss 0.6341339945793152 train acc 0.6685838445807771\n",
            "epoch 1 batch id 4601 loss 0.9736862778663635 train acc 0.6705199956531189\n",
            "epoch 1 batch id 4801 loss 0.5564044713973999 train acc 0.6738700270776922\n",
            "epoch 1 batch id 5001 loss 1.091673731803894 train acc 0.676877124575085\n",
            "epoch 1 batch id 5201 loss 0.8165690898895264 train acc 0.6791362238031148\n",
            "epoch 1 batch id 5401 loss 0.5530579090118408 train acc 0.6812395852619885\n",
            "epoch 1 batch id 5601 loss 1.0086085796356201 train acc 0.6835386538118193\n",
            "epoch 1 batch id 5801 loss 0.7257322072982788 train acc 0.6858515773142562\n",
            "epoch 1 batch id 6001 loss 0.33727264404296875 train acc 0.6877499583402766\n",
            "epoch 1 batch id 6201 loss 0.44109275937080383 train acc 0.6895762780196742\n",
            "epoch 1 batch id 6401 loss 1.0819276571273804 train acc 0.6912103577566006\n",
            "epoch 1 batch id 6601 loss 0.6730824708938599 train acc 0.6931620209059234\n",
            "epoch 1 batch id 6801 loss 0.90753173828125 train acc 0.6945669754447875\n",
            "epoch 1 batch id 7001 loss 0.742933988571167 train acc 0.6964005142122554\n",
            "epoch 1 batch id 7201 loss 0.35650333762168884 train acc 0.6980106929593112\n",
            "epoch 1 batch id 7401 loss 1.1973167657852173 train acc 0.6991538305634374\n",
            "epoch 1 batch id 7601 loss 0.6591012477874756 train acc 0.7005986054466518\n",
            "epoch 1 batch id 7801 loss 0.9659122228622437 train acc 0.7016488270734521\n",
            "epoch 1 batch id 8001 loss 0.6499732732772827 train acc 0.7031464816897888\n",
            "epoch 1 batch id 8201 loss 0.9673812985420227 train acc 0.7042586269967077\n",
            "epoch 1 batch id 8401 loss 0.6831899285316467 train acc 0.705600523747173\n",
            "epoch 1 batch id 8601 loss 1.2320436239242554 train acc 0.7068727473549588\n",
            "epoch 1 train acc 0.7072232482973566\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1323395d47ba46938c1ea3d2f9aba657",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2166 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 test acc 0.767590027700831\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04fd80bc368e4d48b254ed8479e85501",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8663 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 1 loss 1.1429898738861084 train acc 0.6875\n",
            "epoch 2 batch id 201 loss 0.6607110500335693 train acc 0.7496890547263682\n",
            "epoch 2 batch id 401 loss 1.2003166675567627 train acc 0.75\n",
            "epoch 2 batch id 601 loss 0.994924008846283 train acc 0.7533277870216306\n",
            "epoch 2 batch id 801 loss 0.9550855755805969 train acc 0.7548377028714107\n",
            "epoch 2 batch id 1001 loss 0.3032655119895935 train acc 0.7561188811188811\n",
            "epoch 2 batch id 1201 loss 0.8331672549247742 train acc 0.7597314737718568\n",
            "epoch 2 batch id 1401 loss 0.3211359679698944 train acc 0.7625802997858673\n",
            "epoch 2 batch id 1601 loss 0.5073190927505493 train acc 0.7652638975640225\n",
            "epoch 2 batch id 1801 loss 0.5893868803977966 train acc 0.7670391449194892\n",
            "epoch 2 batch id 2001 loss 0.6329295635223389 train acc 0.7685219890054973\n",
            "epoch 2 batch id 2201 loss 0.3281526267528534 train acc 0.7691106315311222\n",
            "epoch 2 batch id 2401 loss 1.575828194618225 train acc 0.7700957934194086\n",
            "epoch 2 batch id 2601 loss 0.6894496083259583 train acc 0.7732843137254902\n",
            "epoch 2 batch id 2801 loss 0.43828755617141724 train acc 0.7740315958586219\n",
            "epoch 2 batch id 3001 loss 0.6115878820419312 train acc 0.7759496834388537\n",
            "epoch 2 batch id 3201 loss 0.2666521668434143 train acc 0.7775695095282724\n",
            "epoch 2 batch id 3401 loss 0.8113448023796082 train acc 0.7782637459570715\n",
            "epoch 2 batch id 3601 loss 0.5758704543113708 train acc 0.7792800610941405\n",
            "epoch 2 batch id 3801 loss 0.413977712392807 train acc 0.7809129176532491\n",
            "epoch 2 batch id 4001 loss 0.7155852317810059 train acc 0.7825231192201949\n",
            "epoch 2 batch id 4201 loss 0.48433631658554077 train acc 0.7838312306593668\n",
            "epoch 2 batch id 4401 loss 0.488680899143219 train acc 0.784665416950693\n",
            "epoch 2 batch id 4601 loss 0.4753560721874237 train acc 0.7846935448815475\n",
            "epoch 2 batch id 4801 loss 0.28502610325813293 train acc 0.7863075400958134\n",
            "epoch 2 batch id 5001 loss 0.8819441199302673 train acc 0.7876549690061988\n",
            "epoch 2 batch id 5201 loss 0.6791206002235413 train acc 0.7889468371467026\n",
            "epoch 2 batch id 5401 loss 0.5570298433303833 train acc 0.7900273097574523\n",
            "epoch 2 batch id 5601 loss 0.7617022395133972 train acc 0.7906512229958936\n",
            "epoch 2 batch id 5801 loss 0.4377244710922241 train acc 0.7916738493363213\n",
            "epoch 2 batch id 6001 loss 0.3499950170516968 train acc 0.7924095984002666\n",
            "epoch 2 batch id 6201 loss 0.2467362880706787 train acc 0.7934103370424125\n",
            "epoch 2 batch id 6401 loss 0.7333810329437256 train acc 0.7940653804093111\n",
            "epoch 2 batch id 6601 loss 0.3960905969142914 train acc 0.7950973337373125\n",
            "epoch 2 batch id 6801 loss 0.8129547834396362 train acc 0.7955815321276283\n",
            "epoch 2 batch id 7001 loss 0.460546612739563 train acc 0.796296957577489\n",
            "epoch 2 batch id 7201 loss 0.21145397424697876 train acc 0.7969205665879739\n",
            "epoch 2 batch id 7401 loss 0.8302397727966309 train acc 0.7973584650722876\n",
            "epoch 2 batch id 7601 loss 0.42720159888267517 train acc 0.7979295487435863\n",
            "epoch 2 batch id 7801 loss 0.5384097695350647 train acc 0.7982870785796693\n",
            "epoch 2 batch id 8001 loss 0.45346105098724365 train acc 0.7990798025246845\n",
            "epoch 2 batch id 8201 loss 0.7431273460388184 train acc 0.7993842214364102\n",
            "epoch 2 batch id 8401 loss 0.3402712345123291 train acc 0.7998378169265563\n",
            "epoch 2 batch id 8601 loss 0.9686588048934937 train acc 0.8003357167771189\n",
            "epoch 2 train acc 0.8004285466928316\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1129e9d47824518b34f013cb8452c54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2166 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 test acc 0.7847356879039704\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "train_history=[]\n",
        "test_history=[]\n",
        "loss_history=[]\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "         \n",
        "        #print(label.shape,out.shape)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "            train_history.append(train_acc / (batch_id+1))\n",
        "            loss_history.append(loss.data.cpu().numpy())\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    train_history.append(train_acc / (batch_id+1))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
        "    test_history.append(test_acc / (batch_id+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1_score(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for token_ids, valid_length, segment_ids, label in tqdm(dataloader):\n",
        "            token_ids = token_ids.long().to(device)\n",
        "            segment_ids = segment_ids.long().to(device)\n",
        "            valid_length= valid_length\n",
        "            label = label.long().to(device)\n",
        "            out = model(token_ids, valid_length, segment_ids)\n",
        "            _, preds = torch.max(out, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(label.cpu().numpy())\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')  # macro 평균 사용\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a722915760d4327b636d79c0d817a26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2166 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test F1 score: 0.7728939740860185\n"
          ]
        }
      ],
      "source": [
        "f1 = calculate_f1_score(model, test_dataloader)\n",
        "print(\"Test F1 score:\", f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
